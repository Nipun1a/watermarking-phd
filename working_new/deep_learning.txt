â­ What does your watermarking model actually learn?

Your model has 2 brains:

1ï¸âƒ£ Encoder brain

Its job is to hide a watermark inside the host image.

Output: Watermarked image (should look same as original).

2ï¸âƒ£ Decoder brain

Its job is to extract the watermark back from the watermarked image.

During training, BOTH brains are trained together.

â­ HOW TRAINING STARTS (VERY SIMPLE)

When you do:

history = model.fit(...)


Your combined model learns TWO main things:

ğŸ¯ Goal 1: Make watermarked image look SAME as original

Encoder tries to hide:
âœ” watermark inside image
âŒ without changing the image too much

This uses loss_img:

MSE loss (pixel difference)

SSIM loss (image similarity)

VGG perceptual loss (features match)

This ensures watermarked image:

has high PSNR

has high SSIM

looks very similar

ğŸ¯ Goal 2: Make decoder recover watermark accurately

Decoder learns to pull the hidden watermark out.

This uses loss_wm:

MSE loss

SSIM loss

This ensures:

watermark extraction is accurate

watermark edges and details are preserved

â­ WHAT HAPPENS DURING TRAINING (STEP BY STEP)**
Step 1 â€” Generator sends training data

Your generator provides batches:

Combined Input (image + watermark)
Target Output 1 â†’ original image
Target Output 2 â†’ original watermark


So the model sees both tasks.

Step 2 â€” Encoder embeds watermark

TensorFlow computes:

encoded_image = encoder(input_image_with_watermark)


This image should look almost identical to the real image.

Step 3 â€” Decoder extracts watermark

The encoded image is passed:

decoded_watermark = decoder(encoded_image)


This watermark should match the original watermark.

Step 4 â€” Loss is calculated

Two losses:

Loss 1 â†’ How close encoded image is to original image
Loss 2 â†’ How close decoded watermark is to original watermark


Your code:

loss=[loss_img, loss_wm]


This means:

âœ” Encoder improves â†’ makes watermark invisible
âœ” Decoder improves â†’ extracts watermark perfectly
Step 5 â€” Backpropagation & weight update

The optimizer:

Adam(1e-4)


adjusts the network weights so both tasks improve.

â­ WHY THE MODEL GETS GOOD RESULTS?

Your training includes:

âœ” Data Augmentation
flip left/right
flip up/down
JPEG compression noise


This teaches model to understand:

noise

compression

distortions

âœ” Perceptual Loss (VGG19)

This makes the watermarked image keep:

sharp edges

textures

realistic look

âœ” Residual Blocks (ResNet)

Helps train deeper model without vanishing gradients.

â­ In short, the model learns two things:
ğŸ”µ Encoder learns: How to hide watermark invisibly
ğŸ”´ Decoder learns: How to find the hidden watermark

Both are trained together during model.fit().

ğŸ”¥ VERY SHORT SUMMARY (10 seconds)

When you run model.fit():

The encoder hides the watermark in the image

The decoder extracts the watermark

The model compares:

encoded vs original image

decoded vs original watermark

Loss is calculated

Weights are updated

After 25 epochs â†’ high quality watermarking

If you want, I can draw you a diagram showing the full pipeline:

ğŸ‘‡
â€œinput â†’ encoder â†’ watermarked image â†’ decoder â†’ watermarkâ€

Just say: â€œMake me diagramâ€